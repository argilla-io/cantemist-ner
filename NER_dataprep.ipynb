{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install biome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import spacy\n",
    "import glob\n",
    "from typing import Tuple, List\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer, PretrainedTransformerMismatchedIndexer\n",
    "from allennlp.data import Token, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(txt_files, ann_files, nlp, align_offsets = True):\n",
    "    dfs = []\n",
    "    for txt, ann in tqdm(zip(txt_files, ann_files), total=len(txt_files)):\n",
    "\n",
    "        doc = helper.brat2doc(\n",
    "            Path(txt), \n",
    "            Path(ann), \n",
    "            nlp, \n",
    "            align_offsets=align_offsets, \n",
    "            remove_parents=True, \n",
    "            remove_siblings=True\n",
    "        )\n",
    "        dfs.append(helper.doc2df(doc, Path(txt).name))\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "def find_misaligned_annotations(df, verbose=True):\n",
    "    # find misaligned annotations?\n",
    "    idx = []\n",
    "    for row in df.itertuples():\n",
    "        if '-' in row.labels:\n",
    "            idx.append(row.Index)\n",
    "    return idx\n",
    "\n",
    "def get_destroyer_tokens(df, model_name=\"distilbert-base-multilingual-cased\") -> Tuple[List, List]:\n",
    "    destroyed_tokens = []\n",
    "    destroyed_tags = []\n",
    "    offs = []\n",
    "    indexer = PretrainedTransformerMismatchedIndexer(model_name=model_name)\n",
    "    vocab = Vocabulary()\n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        tokens = [Token(tok) for tok in row.text]\n",
    "        token_indexes = [indexer.tokens_to_indices([tok], vocabulary=vocab) for tok in tokens]\n",
    "        token_ids = [token_index[\"token_ids\"] for token_index in token_indexes]\n",
    "        token_offs = [token_index[\"offsets\"] for token_index in token_indexes]\n",
    "        is_empty = [len(ids) <= 2 for ids in token_ids]\n",
    "        if any(is_empty):\n",
    "            idx = [i for i in range(len(is_empty)) if is_empty[i] is True]\n",
    "            offs += [token_offs[i] for i in idx]\n",
    "            for tok, tag in [(row.text[i], row.labels[i]) for i in idx]:\n",
    "                if tok not in destroyed_tokens:\n",
    "                    destroyed_tokens.append(tok)\n",
    "                if tag not in destroyed_tags:\n",
    "                    destroyed_tags.append(tag)\n",
    "                    \n",
    "    return destroyed_tokens, destroyed_tags, offs\n",
    "\n",
    "def replace_tokens_with_char(df: pd.DataFrame, tokens: List[str], char: str) -> int:\n",
    "    changes = 0\n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        idx = [i for i in range(len(row.text)) if row.text[i] in tokens]\n",
    "        for i in idx:\n",
    "            row.text[i] = char\n",
    "            changes += 1\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = helper.custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data sets without aligning tokens and offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_files_train = sorted(glob.glob(\"data/train-set-to-publish/cantemist-ner/cc*.ann\"))\n",
    "txt_files_train = sorted(glob.glob(\"data/train-set-to-publish/cantemist-ner/cc*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat(create_df(txt_files_train, ann_files_train, nlp, align_offsets=False), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = find_misaligned_annotations(df_train)\n",
    "print(len(idx), idx)\n",
    "df_train.loc[idx, :]\n",
    "df_train.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=4678, text_org='Páncreas y madre fallecida de Ca.', text=['Páncreas', 'y', 'madre', 'fallecida', 'de', 'Ca', '.'], labels=['O', 'O', 'O', 'O', 'O', 'B-MORFOLOGIA_NEOPLASIA', 'L-MORFOLOGIA_NEOPLASIA'], file='cc_onco307.txt', sentence_offset=254)\n",
      "Pandas(Index=4679, text_org='Microcítico de pulmón.\\n', text=['Microcítico', 'de', 'pulmón', '.', '\\n'], labels=['U-MORFOLOGIA_NEOPLASIA', 'O', 'O', 'O', 'O'], file='cc_onco307.txt', sentence_offset=288)\n",
      "Pandas(Index=6099, text_org='-Recaída a distancia con ILE 5.9 años de Ca.', text=['-', 'Recaída', 'a', 'distancia', 'con', 'ILE', '5.9', 'años', 'de', 'Ca', '.'], labels=['O', 'B-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'L-MORFOLOGIA_NEOPLASIA'], file='cc_onco364.txt', sentence_offset=5282)\n",
      "Pandas(Index=6100, text_org='renal de células claras operado sin posibilidad de tratamiento oncoespecífico con antiangiogénicos por mala tolerancia y poca adherencia.\\n\\n', text=['renal', 'de', 'células', 'claras', 'operado', 'sin', 'posibilidad', 'de', 'tratamiento', 'oncoespecífico', 'con', 'antiangiogénicos', 'por', 'mala', 'tolerancia', 'y', 'poca', 'adherencia', '.', '\\n\\n'], labels=['B-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'L-MORFOLOGIA_NEOPLASIA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], file='cc_onco364.txt', sentence_offset=5327)\n",
      "Pandas(Index=14540, text_org='A continuación, se inicia quimioterapia con carboplatino-gemcitabina (por 6 ciclos) para tratar el Ca.', text=['A', 'continuación', ',', 'se', 'inicia', 'quimioterapia', 'con', 'carboplatino', '-', 'gemcitabina', '(', 'por', '6', 'ciclos', ')', 'para', 'tratar', 'el', 'Ca', '.'], labels=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MORFOLOGIA_NEOPLASIA', 'L-MORFOLOGIA_NEOPLASIA'], file='cc_onco825.txt', sentence_offset=5797)\n",
      "Pandas(Index=14541, text_org='epidermoide junto con radioterapia en mediastino a dosis de 50 Gy.', text=['epidermoide', 'junto', 'con', 'radioterapia', 'en', 'mediastino', 'a', 'dosis', 'de', '50', 'Gy', '.'], labels=['U-MORFOLOGIA_NEOPLASIA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], file='cc_onco825.txt', sentence_offset=5900)\n"
     ]
    }
   ],
   "source": [
    "# fix invalid tag sequences caused by our sentence splitting, that is annotations that contain a '.'\n",
    "for row in df_train.itertuples():\n",
    "    if row.labels[-1].startswith(\"B-\"):\n",
    "        row.labels[-1] = \"U-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[-1].startswith(\"I-\"):\n",
    "        row.labels[-1] = \"L-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[0].startswith(\"L-\"):\n",
    "        row.labels[0] = \"U-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[0].startswith(\"I-\"):\n",
    "        row.labels[0] = \"B-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for the BERT tokenizer\n",
    "\n",
    "For some tokens the BERT tokenizer/indexer/embedder returns an empty vector. We replace these \"destroyer tokens\" with a \"friendly char\" that is not looking for a fight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['\\n',\n",
       "  '\\n\\n',\n",
       "  '\\n\\n\\n',\n",
       "  '\\n\\n\\n\\n',\n",
       "  '\\n  \\n',\n",
       "  ' ',\n",
       "  '\\t',\n",
       "  '\\n \\n',\n",
       "  '\\n ',\n",
       "  '\\uf0e8',\n",
       "  '\\uf076'],\n",
       " ['O'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks, tags, offs = get_destroyer_tokens(df_train)\n",
    "toks, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_tokens_with_char(df_train, toks, \"æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for the XLM-R tokenizer\n",
    "\n",
    "For some tokens the XLM-R tokenizer/indexer/embedder returns an empty vector. We replace these \"destroyer tokens\" with a \"friendly char\" that is not looking for a fight. There "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18479/18479 [00:33<00:00, 559.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['\\n', '\\n\\n', '\\n\\n\\n', '\\n\\n\\n\\n', '\\n  \\n', ' ', '\\t', '\\n \\n', '\\n '],\n",
       " ['O'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks, tags, offs = get_destroyer_tokens(df_train, model_name=\"xlm-roberta-base\")\n",
    "toks, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_tokens_with_char(df_train, toks, \"æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dev1 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_files_dev1 = sorted(glob.glob(\"data/dev-set1-to-publish/cantemist-ner/cc*.ann\"))\n",
    "txt_files_dev1 = sorted(glob.glob(\"data/dev-set1-to-publish/cantemist-ner/cc*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:23<00:00, 10.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df_dev1 = pd.concat(create_df(txt_files_dev1, ann_files_dev1, nlp, align_offsets=False), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2145, 3809] 2\n"
     ]
    }
   ],
   "source": [
    "idx = find_misaligned_annotations(df_dev1)\n",
    "print(idx, len(idx))\n",
    "df_dev1.loc[idx, :]\n",
    "df_dev1.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=8738, text_org='Se realiza RMN cerebral que confirma los hallazgos del PET y se remite a Neurocirugía para extirpación de las lesiones bajo el juicio clínico de posibles metástasis de Ca.', text=['Se', 'realiza', 'RMN', 'cerebral', 'que', 'confirma', 'los', 'hallazgos', 'del', 'PET', 'y', 'se', 'remite', 'a', 'Neurocirugía', 'para', 'extirpación', 'de', 'las', 'lesiones', 'bajo', 'el', 'juicio', 'clínico', 'de', 'posibles', 'metástasis', 'de', 'Ca', '.'], labels=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'L-MORFOLOGIA_NEOPLASIA'], file='cc_onco970.txt', sentence_offset=180, entity_text=[])\n",
      "Pandas(Index=8739, text_org='papilar de tiroides.', text=['papilar', 'de', 'tiroides', '.'], labels=['B-MORFOLOGIA_NEOPLASIA', 'I-MORFOLOGIA_NEOPLASIA', 'L-MORFOLOGIA_NEOPLASIA', 'O'], file='cc_onco970.txt', sentence_offset=352, entity_text=[])\n"
     ]
    }
   ],
   "source": [
    "# fix invalid tag sequences caused by our sentence splitting, that is annotations that contain a '.'\n",
    "for row in df_dev1.itertuples():\n",
    "    if row.labels[-1].startswith(\"B-\"):\n",
    "        row.labels[-1] = \"U-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[-1].startswith(\"I-\"):\n",
    "        row.labels[-1] = \"L-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[0].startswith(\"L-\"):\n",
    "        row.labels[0] = \"U-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[0].startswith(\"I-\"):\n",
    "        row.labels[0] = \"B-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for the BERT tokenizer\n",
    "\n",
    "For some tokens the BERT tokenizer/indexer/embedder returns an empty vector. We replace these \"destroyer tokens\" with a \"friendly char\" that is not looking for a fight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9073/9073 [00:14<00:00, 604.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['\\n', '\\n\\n', '\\n\\n\\n', ' ', '\\uf0a7'], ['O'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks, tags, offs = get_destroyer_tokens(df_dev1)\n",
    "toks, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9073/9073 [00:00<00:00, 148099.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5903"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_tokens_with_char(df_dev1, toks, \"æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dev2 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_files_dev2 = sorted(glob.glob(\"data/dev-set2-to-publish/cantemist-ner/cc*.ann\"))\n",
    "txt_files_dev2 = sorted(glob.glob(\"data/dev-set2-to-publish/cantemist-ner/cc*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:54<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df_dev2 = pd.concat(create_df(txt_files_dev2, ann_files_dev2, nlp, align_offsets=False), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[807, 2233, 3739, 4934, 6334, 6337, 6682, 6956] 8\n"
     ]
    }
   ],
   "source": [
    "idx = find_misaligned_annotations(df_dev2)\n",
    "print(idx, len(idx))\n",
    "df_dev2.loc[idx, :]\n",
    "df_dev2.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix invalid tag sequences caused by our sentence splitting, that is annotations that contain a '.'\n",
    "for row in df_dev2.itertuples():\n",
    "    if row.labels[-1].startswith(\"B-\"):\n",
    "        row.labels[-1] = \"U-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[-1].startswith(\"I-\"):\n",
    "        row.labels[-1] = \"L-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[0].startswith(\"L-\"):\n",
    "        row.labels[0] = \"U-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)\n",
    "    if row.labels[0].startswith(\"I-\"):\n",
    "        row.labels[0] = \"B-MORFOLOGIA_NEOPLASIA\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for the BERT tokenizer\n",
    "\n",
    "For some tokens the BERT tokenizer/indexer/embedder returns an empty vector. We replace these \"destroyer tokens\" with a \"friendly char\" that is not looking for a fight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['\\n', '\\n\\n', '\\n ', '\\n\\n\\n'], ['O'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks, tags, offs = get_destroyer_tokens(df_dev2)\n",
    "toks, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5385"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_tokens_with_char(df_dev2, toks, \"æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev2.to_json(\"data/NER_david/dev2_wo_align.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test set (for the language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files_test = sorted(glob.glob(\"data/test-background-set-to-publish/*.txt\"))\n",
    "txt_paths_test = [Path(txt_file) for txt_file in txt_files_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5232/5232 [03:56<00:00, 22.14it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for txt_path in tqdm(txt_paths_test):\n",
    "    text = txt_path.read_text()\n",
    "    doc = nlp(text)\n",
    "    dfs.append(helper.doc2df(doc, txt_path.name))\n",
    "               \n",
    "df_test = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for the BERT tokenizer\n",
    "\n",
    "For some tokens the BERT tokenizer/indexer/embedder returns an empty vector. We replace these \"destroyer tokens\" with a \"friendly char\" that is not looking for a fight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87623/87623 [02:28<00:00, 589.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['\\n',\n",
       "  '\\n\\n',\n",
       "  '\\n\\n\\n',\n",
       "  '\\x85',\n",
       "  '\\x85 ',\n",
       "  '\\x99',\n",
       "  '\\n\\xa0\\n',\n",
       "  '\\n\\n\\xa0\\n',\n",
       "  '\\xad',\n",
       "  '\\n\\n\\n\\n',\n",
       "  '\\n\\n\\n\\n\\n\\n',\n",
       "  '�',\n",
       "  '\\n \\n',\n",
       "  ' \\n\\n',\n",
       "  '\\n\\n \\n',\n",
       "  '\\n\\n \\n\\n',\n",
       "  ' ',\n",
       "  '  ',\n",
       "  '\\n\\n ',\n",
       "  '\\n \\n ',\n",
       "  '\\n\\n \\n\\n ',\n",
       "  '  \\n\\n',\n",
       "  '\\u2028\\n\\n',\n",
       "  '\\u2028      \\n\\n',\n",
       "  '\\u2028\\u2028\\n',\n",
       "  '                        \\n\\n',\n",
       "  '\\n\\n  ',\n",
       "  '\\n  ',\n",
       "  '\\n  \\n  \\n',\n",
       "  '\\n \\n\\n',\n",
       "  '\\n  \\n',\n",
       "  '\\n  \\n\\n',\n",
       "  '\\n \\n \\n',\n",
       "  '\\n ',\n",
       "  '\\t',\n",
       "  '\\t\\t',\n",
       "  '\\t\\t\\n',\n",
       "  '\\t\\t\\t'],\n",
       " ['_SP', 'PROPN___', 'NUM__NumForm=Digit'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks, tags, offs = get_destroyer_tokens(df_test)\n",
    "toks, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87623/87623 [00:01<00:00, 65701.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33039"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_tokens_with_char(df_test, toks, \"æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json(\"data/NER/train_for_bert.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev1.to_json(\"data/NER/dev1_for_bert.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(\"data/NER/test_for_bert.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('biome': conda)",
   "language": "python",
   "name": "python37764bitbiomeconda7f3b5c32f8a548baa7dbf61016404797"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
